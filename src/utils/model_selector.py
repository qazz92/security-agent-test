"""
Dual Model Selector for Cost Optimization
ì¶”ë¡  ëª¨ë¸(Thinking) vs ì§€ì‹œ ëª¨ë¸(Instruct) ìë™ ì„ íƒ
"""

import os
import logging
from enum import Enum
from typing import Dict, Any, Optional
from langchain_openai import ChatOpenAI

logger = logging.getLogger(__name__)


class TaskComplexity(Enum):
    """ì‘ì—… ë³µì¡ë„"""

    # ë³µì¡í•œ ì¶”ë¡  í•„ìš” (Thinking Model)
    CRITICAL_ANALYSIS = "critical_analysis"      # ì·¨ì•½ì  ì‹¬ê°ë„ í‰ê°€
    RISK_ASSESSMENT = "risk_assessment"          # ë¹„ì¦ˆë‹ˆìŠ¤ ë¦¬ìŠ¤í¬ í‰ê°€
    VULNERABILITY_TRIAGE = "vulnerability_triage" # ì·¨ì•½ì  ìš°ì„ ìˆœìœ„ ê²°ì •
    SECURITY_DESIGN = "security_design"          # ë³´ì•ˆ ì„¤ê³„ ë° ì•„í‚¤í…ì²˜
    ROOT_CAUSE_ANALYSIS = "root_cause_analysis"  # ê·¼ë³¸ ì›ì¸ ë¶„ì„

    # ë‹¨ìˆœ ì‹¤í–‰ (Instruct Model)
    TOOL_CALLING = "tool_calling"                # Tool í˜¸ì¶œ ê²°ì •
    DATA_FORMATTING = "data_formatting"          # ë°ì´í„° í¬ë§· ë³€í™˜
    TEMPLATE_GENERATION = "template_generation"  # í…œí”Œë¦¿ ìƒì„±
    CODE_FORMATTING = "code_formatting"          # ì½”ë“œ í¬ë§·íŒ…
    SIMPLE_EXTRACTION = "simple_extraction"      # ë‹¨ìˆœ ë°ì´í„° ì¶”ì¶œ


class ModelConfig:
    """ëª¨ë¸ ì„¤ì •"""

    # Thinking Model (ë³µì¡í•œ ì¶”ë¡ )
    THINKING = {
        "model": os.getenv('MODEL_THINKING', 'qwen/qwen3-next-80b-a3b-thinking'),
        "temperature": 0.2,  # ë” ë³´ìˆ˜ì 
        "max_tokens": 16384,  # ê¸´ ì¶”ë¡  ê°€ëŠ¥ (ë°°ì¹˜ ì²˜ë¦¬ ê³ ë ¤)
        "description": "Complex reasoning and analysis"
    }

    # Instruct Model (ë‹¨ìˆœ ì‹¤í–‰)
    INSTRUCT = {
        "model": os.getenv('MODEL_INSTRUCT', 'qwen/qwen3-next-80b-a3b-instruct'),
        "temperature": 0.1,  # ê²°ì •ì 
        "max_tokens": 8192,  # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì¶©ë¶„í•œ í¬ê¸° (Qwen3-Next: 256K context window)
        "description": "Tool calling and simple tasks"
    }


class ModelSelector:
    """
    ì‘ì—… ìœ í˜•ì— ë”°ë¼ ìµœì ì˜ ëª¨ë¸ ì„ íƒ

    ì‚¬ìš© ì˜ˆ:
        selector = ModelSelector()
        llm = selector.get_llm(TaskComplexity.CRITICAL_ANALYSIS)
    """

    # ì‘ì—…ë³„ ëª¨ë¸ ë§¤í•‘
    TASK_MODEL_MAPPING = {
        # Thinking Model ì‚¬ìš© (ë³µì¡í•œ ì¶”ë¡ )
        # OpenRouter reasoning parameter ì§€ì› (2025-09-30 í™•ì¸)
        # LiteLLM 1.74.9+ supports reasoning_content extraction
        TaskComplexity.CRITICAL_ANALYSIS: "thinking",
        TaskComplexity.RISK_ASSESSMENT: "thinking",
        TaskComplexity.VULNERABILITY_TRIAGE: "thinking",
        TaskComplexity.SECURITY_DESIGN: "thinking",
        TaskComplexity.ROOT_CAUSE_ANALYSIS: "thinking",

        # Instruct Model ì‚¬ìš© (ë‹¨ìˆœ ì‹¤í–‰ ì‘ì—…)
        TaskComplexity.TOOL_CALLING: "instruct",
        TaskComplexity.DATA_FORMATTING: "instruct",
        TaskComplexity.TEMPLATE_GENERATION: "instruct",
        TaskComplexity.CODE_FORMATTING: "instruct",
        TaskComplexity.SIMPLE_EXTRACTION: "instruct",
    }

    def __init__(self, api_key: str = None, api_base: str = None):
        self.api_key = api_key or os.getenv('OPENROUTER_API_KEY')
        self.api_base = api_base or os.getenv('OPENAI_API_BASE', 'https://openrouter.ai/api/v1')

        # ë¹„ìš© ì¶”ì 
        self.usage_stats = {
            "thinking": {"calls": 0, "tokens": 0},
            "instruct": {"calls": 0, "tokens": 0}
        }

    def get_llm(
        self,
        task_complexity: TaskComplexity,
        callbacks: list = None,
        override_params: Dict[str, Any] = None
    ) -> ChatOpenAI:
        """
        ì‘ì—… ë³µì¡ë„ì— ë”°ë¼ ì ì ˆí•œ LLM ë°˜í™˜

        Args:
            task_complexity: ì‘ì—… ë³µì¡ë„ (Enum)
            callbacks: LangChain callbacks (Langfuse ë“±)
            override_params: ëª¨ë¸ íŒŒë¼ë¯¸í„° ì˜¤ë²„ë¼ì´ë“œ

        Returns:
            ì„¤ì •ëœ ChatOpenAI ì¸ìŠ¤í„´ìŠ¤
        """

        # ëª¨ë¸ íƒ€ì… ê²°ì •
        model_type = self.TASK_MODEL_MAPPING.get(task_complexity, "instruct")
        config = ModelConfig.THINKING if model_type == "thinking" else ModelConfig.INSTRUCT

        # íŒŒë¼ë¯¸í„° ì„¤ì •
        # CrewAI/LiteLLM requires 'openrouter/' prefix, but Langfuse doesn't accept it
        # Use prefix for API calls, but store original name for Langfuse metadata
        base_model_name = config['model']
        model_name_with_prefix = f"openrouter/{base_model_name}"

        params = {
            "model": model_name_with_prefix,  # CrewAI/LiteLLM needs prefix
            "temperature": config["temperature"],
            "max_tokens": config["max_tokens"],
            "openai_api_key": self.api_key,
            "openai_api_base": self.api_base,
            "callbacks": callbacks or [],
            "model_kwargs": {
                "extra_headers": {
                    "HTTP-Referer": "https://github.com/security-agent-portfolio",
                    "X-Title": f"SecurityAgent-{task_complexity.value}",
                },
                # Pass original model name for Langfuse (without prefix)
                "metadata": {
                    "model_name": base_model_name,  # Langfuse will use this
                }
            }
        }

        # OpenRouter Thinking ëª¨ë¸ ì„¤ì •: reasoning parameter ì¶”ê°€
        if model_type == "thinking":
            params["model_kwargs"]["extra_body"] = {
                "reasoning": {
                    "max_tokens": 2000,  # ì¶”ë¡  ê³¼ì •ì— ì¶©ë¶„í•œ í† í° ì œê³µ
                    "enabled": True      # ëª…ì‹œì ìœ¼ë¡œ reasoning í™œì„±í™”
                }
            }
            logger.info(f"   ğŸ§  Reasoning enabled: max_tokens=2000")

        # ì˜¤ë²„ë¼ì´ë“œ ì ìš©
        if override_params:
            params.update(override_params)

        # í†µê³„ ì—…ë°ì´íŠ¸
        self.usage_stats[model_type]["calls"] += 1

        # ë¡œê¹…
        logger.info(
            f"ğŸ¤– Selected {model_type.upper()} model for {task_complexity.value}: "
            f"{config['model']} (temp={config['temperature']})"
        )
        logger.info(f"   ğŸ“ Callbacks: {len(callbacks or [])} configured")

        llm = ChatOpenAI(**params)

        # Verify callbacks are set
        if hasattr(llm, 'callbacks') and llm.callbacks:
            logger.info(f"   âœ… LLM callbacks verified: {len(llm.callbacks)} callbacks")
        elif callbacks:
            logger.warning(f"   âš ï¸ Callbacks provided but not set on LLM!")

        return llm

    def get_model_for_agent(self, agent_name: str, callbacks: list = None) -> ChatOpenAI:
        """
        Agent ì´ë¦„ì— ë”°ë¼ ì ì ˆí•œ ëª¨ë¸ ë°˜í™˜ (í¸ì˜ í•¨ìˆ˜)

        Args:
            agent_name: 'security_analyst', 'triage_specialist', 'remediation_engineer'
            callbacks: LangChain callbacks

        Returns:
            ì„¤ì •ëœ ChatOpenAI ì¸ìŠ¤í„´ìŠ¤
        """

        agent_task_mapping = {
            "security_analyst": TaskComplexity.CRITICAL_ANALYSIS,  # Thinking
            "triage_specialist": TaskComplexity.VULNERABILITY_TRIAGE,  # Thinking
            "remediation_engineer": TaskComplexity.TOOL_CALLING,  # Instruct
        }

        task = agent_task_mapping.get(agent_name, TaskComplexity.TOOL_CALLING)
        return self.get_llm(task, callbacks)

    def get_usage_report(self) -> Dict[str, Any]:
        """ë¹„ìš© ìµœì í™” ë³´ê³ ì„œ"""

        thinking_calls = self.usage_stats["thinking"]["calls"]
        instruct_calls = self.usage_stats["instruct"]["calls"]
        total_calls = thinking_calls + instruct_calls

        if total_calls == 0:
            return {"message": "No LLM calls yet"}

        # ì˜ˆìƒ ë¹„ìš© (ê°€ìƒì˜ ê°€ê²© - OpenRouter ì‹¤ì œ ê°€ê²©ìœ¼ë¡œ ëŒ€ì²´ í•„ìš”)
        THINKING_COST_PER_CALL = 0.02  # $0.02 per call (ì˜ˆì‹œ)
        INSTRUCT_COST_PER_CALL = 0.005  # $0.005 per call (ì˜ˆì‹œ)

        thinking_cost = thinking_calls * THINKING_COST_PER_CALL
        instruct_cost = instruct_calls * INSTRUCT_COST_PER_CALL
        total_cost = thinking_cost + instruct_cost

        # ë§Œì•½ ì „ë¶€ Thinking ëª¨ë¸ì„ ì‚¬ìš©í–ˆë‹¤ë©´?
        all_thinking_cost = total_calls * THINKING_COST_PER_CALL
        savings = all_thinking_cost - total_cost
        savings_pct = (savings / all_thinking_cost * 100) if all_thinking_cost > 0 else 0

        return {
            "total_calls": total_calls,
            "thinking_calls": thinking_calls,
            "instruct_calls": instruct_calls,
            "thinking_percentage": round(thinking_calls / total_calls * 100, 1),
            "instruct_percentage": round(instruct_calls / total_calls * 100, 1),
            "estimated_cost": {
                "thinking": round(thinking_cost, 4),
                "instruct": round(instruct_cost, 4),
                "total": round(total_cost, 4),
                "currency": "USD"
            },
            "cost_savings": {
                "amount": round(savings, 4),
                "percentage": round(savings_pct, 1),
                "vs": "all-thinking-model"
            }
        }

    def print_usage_report(self):
        """ë¹„ìš© ë³´ê³ ì„œ ì¶œë ¥"""
        report = self.get_usage_report()

        if "message" in report:
            logger.info(report["message"])
            return

        logger.info("="*70)
        logger.info("ğŸ’° MODEL USAGE & COST OPTIMIZATION REPORT")
        logger.info("="*70)
        logger.info(f"Total LLM Calls: {report['total_calls']}")
        logger.info(f"  ğŸ§  Thinking Model: {report['thinking_calls']} ({report['thinking_percentage']}%)")
        logger.info(f"  âš¡ Instruct Model: {report['instruct_calls']} ({report['instruct_percentage']}%)")
        logger.info("")
        logger.info(f"Estimated Cost:")
        logger.info(f"  Thinking: ${report['estimated_cost']['thinking']}")
        logger.info(f"  Instruct: ${report['estimated_cost']['instruct']}")
        logger.info(f"  Total: ${report['estimated_cost']['total']}")
        logger.info("")
        logger.info(f"ğŸ’µ Cost Savings vs All-Thinking:")
        logger.info(f"  Amount: ${report['cost_savings']['amount']}")
        logger.info(f"  Percentage: {report['cost_savings']['percentage']}%")
        logger.info("="*70)


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_model_selector = None


def get_model_selector() -> ModelSelector:
    """ì „ì—­ ModelSelector ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _model_selector
    if _model_selector is None:
        _model_selector = ModelSelector()
    return _model_selector