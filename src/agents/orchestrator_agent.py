"""
ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì—ì´ì „íŠ¸
CrewAI ê¸°ë°˜ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ë³´ì•ˆ ë¶„ì„ ì›Œí¬í”Œë¡œìš° ì¡°ìœ¨
"""

import asyncio
import time
import json
import logging
import os
import re
from typing import Dict, List, Any, Optional
from datetime import datetime

from .security_crew import SecurityCrewManager
from ..models.llm_config import create_security_llm, SecurityModelSelector

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)


class SecurityOrchestrator:
    """CrewAI ê¸°ë°˜ ë³´ì•ˆ ë¶„ì„ ë° ìˆ˜ì • ì›Œí¬í”Œë¡œìš° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°"""

    def __init__(self, verbose: bool = True):
        """
        Args:
            verbose: ìƒì„¸ ë¡œê¹… í™œì„±í™”
        """
        self.verbose = verbose

        # CrewAI ë°©ì‹: ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ìë™ í˜‘ì—…
        logger.info("ğŸ¤– Using CrewAI for multi-agent orchestration")
        self.crew_manager = SecurityCrewManager(verbose=verbose)

        # ì „ì²´ ì›Œí¬í”Œë¡œìš° ê²°ê³¼ ì €ì¥
        self.workflow_results = {}
        self.performance_metrics = {
            "workflow_start_time": None,
            "workflow_end_time": None,
            "total_duration": None,
            "phases_completed": [],
            "agents_used": [],
            "orchestration_mode": "crewai"
        }

    async def analyze_and_remediate(
        self,
        project_path: str,
        user_query: str = "Comprehensive security analysis and remediation"
    ) -> Dict[str, Any]:
        """CrewAI ê¸°ë°˜ ë³´ì•ˆ ë¶„ì„ ë° ìˆ˜ì • ë°©ì•ˆ ìƒì„± ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""

        # ğŸš€ FAST_TEST_MODE: ìºì‹œëœ ê²°ê³¼ ì‚¬ìš© (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)
        if os.environ.get('FAST_TEST_MODE') == 'true':
            cached_result = self._load_last_result()
            if cached_result:
                logger.info("âš¡ FAST_TEST_MODE: Using cached result (skipping analysis)")
                return cached_result

        self.performance_metrics["workflow_start_time"] = time.time()

        try:
            logger.info("ğŸ¤– Starting CrewAI-based security analysis...")

            # CrewAI Crew ì‹¤í–‰ (ë¹„ë™ê¸° ë˜í¼)
            loop = asyncio.get_event_loop()
            github_repo_url = os.environ.get('GITHUB_REPO_URL', 'https://github.com/qazz92/security-agent-test')

            crew_result = await loop.run_in_executor(
                None,
                lambda: self.crew_manager.analyze_project(project_path, github_repo_url)
            )

            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self.performance_metrics["workflow_end_time"] = time.time()
            self.performance_metrics["total_duration"] = (
                self.performance_metrics["workflow_end_time"] -
                self.performance_metrics["workflow_start_time"]
            )
            self.performance_metrics["agents_used"] = crew_result.get("agents_used", [])
            self.performance_metrics["phases_completed"] = ["crewai_analysis"]

            # CrewAI ê²°ê³¼ë¥¼ Streamlit í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            raw_result = crew_result.get("result", "")
            # CrewOutput ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            if hasattr(raw_result, 'raw'):
                raw_result_str = str(raw_result.raw)
            elif hasattr(raw_result, '__str__'):
                raw_result_str = str(raw_result)
            else:
                raw_result_str = ""

            parsed_result = self._parse_crewai_result(raw_result_str)

            # Streamlit í˜¸í™˜ êµ¬ì¡°ë¡œ êµ¬ì¡°í™”
            workflow_results = {
                "workflow_metadata": {
                    "project_path": project_path,
                    "user_query": user_query,
                    "analysis_timestamp": datetime.now().isoformat(),
                    "workflow_version": "2.0-crewai",
                    "orchestration_mode": "crewai",
                    "agents_used": crew_result.get("agents_used", [])
                },
                "security_analysis": parsed_result.get("security_analysis", {}),
                "remediation_plan": parsed_result.get("remediation_plan", {}),
                "final_report": parsed_result.get("final_report", {}),
                "executive_summary": parsed_result.get("executive_summary", raw_result_str[:500] + "..."),
                "crewai_result": raw_result_str,
                "success": crew_result.get("success", False),
                "performance_metrics": self.performance_metrics,
                "github_repo_url": github_repo_url
            }

            self.workflow_results = workflow_results

            # ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)
            self._save_last_result(workflow_results)

            logger.info("âœ… CrewAI analysis completed successfully")
            logger.info(f"â±ï¸ Total duration: {self.performance_metrics['total_duration']:.1f}s")

            return workflow_results

        except Exception as e:
            logger.error(f"âŒ CrewAI analysis failed: {e}")
            return self._create_error_result(
                f"CrewAI workflow execution failed: {str(e)}",
                project_path
            )

    def _parse_crewai_result(self, raw_result: str) -> Dict[str, Any]:
        """CrewAI ê²°ê³¼ë¥¼ êµ¬ì¡°í™”ëœ JSONìœ¼ë¡œ ë³€í™˜ (JSON ìš°ì„ , Fallback: Regex)"""

        # ê¸°ë³¸ êµ¬ì¡°
        parsed = {
            "security_analysis": {
                "vulnerabilities": [],
                "analysis_summary": {
                    "total_vulnerabilities": 0,
                    "severity_distribution": {},
                    "analysis_duration": self.performance_metrics.get("total_duration", 0)
                }
            },
            "remediation_plan": {},
            "final_report": {},
            "executive_summary": ""
        }

        try:
            import json

            agent_json = None

            # ğŸš€ Simple approach: Find ```json ... ``` block
            if "```json" in raw_result:
                start_marker = "```json"
                end_marker = "```"

                start_idx = raw_result.find(start_marker)
                if start_idx != -1:
                    # Find the end marker after start_marker
                    start_idx += len(start_marker)
                    end_idx = raw_result.find(end_marker, start_idx)

                    if end_idx != -1:
                        json_text = raw_result[start_idx:end_idx].strip()

                        try:
                            agent_json = json.loads(json_text)
                            logger.info("âœ… Found JSON in markdown block (```json ... ```)")
                        except json.JSONDecodeError as e:
                            logger.warning(f"âš ï¸ JSON parsing failed at position {e.pos}: {e.msg}")
                            logger.debug(f"   JSON snippet around error: {json_text[max(0, e.pos-50):e.pos+50]}")

            # Fallback: Try parsing entire response as JSON
            if agent_json is None:
                try:
                    agent_json = json.loads(raw_result)
                    logger.info("âœ… Parsed entire response as JSON (no markdown wrapper)")
                except json.JSONDecodeError as e:
                    logger.warning(f"âš ï¸ No valid JSON found in Agent output: {e.msg}")

            if agent_json:

                # JSON ë°ì´í„° ì§ì ‘ ì‚¬ìš©
                summary = agent_json.get("summary", {})
                vulnerabilities = agent_json.get("vulnerabilities", [])

                # ì·¨ì•½ì  ë¦¬ìŠ¤íŠ¸ ë³€í™˜
                for vuln in vulnerabilities:
                    parsed["security_analysis"]["vulnerabilities"].append({
                        "type": vuln.get("type", "Unknown"),
                        "file": vuln.get("file", "unknown"),
                        "severity": vuln.get("severity", "HIGH"),
                        "line": 0,
                        "description": vuln.get("description", ""),
                        "code": vuln.get("before_code", "N/A")
                    })

                # Summary ì§ì ‘ ì‚¬ìš©
                total_vulns = summary.get("total_vulnerabilities", len(vulnerabilities))
                critical_count = summary.get("critical_count", 0)
                high_count = summary.get("high_count", 0)
                medium_count = summary.get("medium_count", 0)
                low_count = summary.get("low_count", 0)

                parsed["security_analysis"]["analysis_summary"] = {
                    "total_vulnerabilities": total_vulns,
                    "severity_distribution": {
                        "CRITICAL": critical_count,
                        "HIGH": high_count,
                        "MEDIUM": medium_count,
                        "LOW": low_count
                    },
                    "analysis_duration": self.performance_metrics.get("total_duration", 0)
                }

                # PR í…œí”Œë¦¿
                pr_template = agent_json.get("pr_template", raw_result)
                parsed["executive_summary"] = pr_template[:1000] if len(pr_template) > 1000 else pr_template

            else:
                # ğŸ”„ Fallback: Regex íŒŒì‹± (ê¸°ì¡´ ë°©ì‹)
                logger.warning("âš ï¸ No JSON found, falling back to regex parsing")

                vuln_headers = re.findall(r'###\s*(\d+)\.\s*([^\n]+)', raw_result)

                for idx, full_title in vuln_headers:
                    match = re.match(r'([^\(]+)\s*\(([^\)]+)\)', full_title)
                    if match:
                        vuln_type = match.group(1).strip()
                        file_info = match.group(2).strip()
                    else:
                        vuln_type = full_title.strip()
                        file_info = "unknown"

                    vuln_type_lower = vuln_type.lower()
                    if any(keyword in vuln_type_lower for keyword in ['sql injection', 'command injection', 'hardcoded', 'deserialization']):
                        severity = "CRITICAL"
                    elif any(keyword in vuln_type_lower for keyword in ['xss', 'ssrf', 'xxe', 'path traversal', 'open redirect']):
                        severity = "HIGH"
                    elif any(keyword in vuln_type_lower for keyword in ['weak crypto', 'csrf']):
                        severity = "MEDIUM"
                    else:
                        severity = "HIGH"

                    parsed["security_analysis"]["vulnerabilities"].append({
                        "type": vuln_type,
                        "file": file_info,
                        "severity": severity,
                        "line": 0,
                        "description": f"{vuln_type} vulnerability in {file_info}",
                        "code": "N/A"
                    })

                # Severity ê³„ì‚°
                severity_counts = {"CRITICAL": 0, "HIGH": 0, "MEDIUM": 0, "LOW": 0}
                for vuln in parsed["security_analysis"]["vulnerabilities"]:
                    severity_counts[vuln["severity"]] += 1

                total_vulns = len(parsed["security_analysis"]["vulnerabilities"])
                critical_count = severity_counts["CRITICAL"]
                high_count = severity_counts["HIGH"]
                medium_count = severity_counts["MEDIUM"]
                low_count = severity_counts["LOW"]

                parsed["security_analysis"]["analysis_summary"] = {
                    "total_vulnerabilities": total_vulns,
                    "severity_distribution": severity_counts,
                    "analysis_duration": self.performance_metrics.get("total_duration", 0)
                }

                parsed["executive_summary"] = raw_result[:1000] + "..."

            # ê³µí†µ: ë³´ì•ˆ ì ìˆ˜ ë° ë¦¬í¬íŠ¸ ìƒì„±
            critical_count = parsed["security_analysis"]["analysis_summary"]["severity_distribution"]["CRITICAL"]
            high_count = parsed["security_analysis"]["analysis_summary"]["severity_distribution"]["HIGH"]
            medium_count = parsed["security_analysis"]["analysis_summary"]["severity_distribution"]["MEDIUM"]
            low_count = parsed["security_analysis"]["analysis_summary"]["severity_distribution"]["LOW"]
            total_vulns = parsed["security_analysis"]["analysis_summary"]["total_vulnerabilities"]

            security_score = max(0, 100 - (critical_count * 25 + high_count * 10 + medium_count * 5 + low_count * 2))
            risk_level = "CRITICAL" if critical_count > 0 else "HIGH" if high_count > 0 else "MEDIUM" if medium_count > 0 else "LOW"

            parsed["remediation_plan"] = {
                "remediation_summary": {
                    "fixes_generated": total_vulns,
                    "pr_template_created": True,
                    "estimated_effort": {"total_hours": total_vulns * 2}
                },
                "detailed_remediation": {
                    "pr_template": raw_result
                }
            }

            parsed["final_report"] = {
                "security_posture": {
                    "overall_score": security_score,
                    "risk_level": risk_level,
                    "vulnerabilities_summary": {
                        "immediate_action_required": critical_count > 0
                    }
                },
                "recommendations": [
                    f"ì¦‰ì‹œ {critical_count}ê°œ Critical ì·¨ì•½ì  ìˆ˜ì •" if critical_count > 0 else None,
                    f"{high_count}ê°œ High ì·¨ì•½ì  7ì¼ ë‚´ ìˆ˜ì •" if high_count > 0 else None,
                    "ì •ê¸°ì ì¸ ë³´ì•ˆ ìŠ¤ìº” ë„ì…",
                    "ê°œë°œíŒ€ ë³´ì•ˆ êµìœ¡ ì‹¤ì‹œ"
                ]
            }
            parsed["final_report"]["recommendations"] = [r for r in parsed["final_report"]["recommendations"] if r]

        except Exception as e:
            logger.error(f"âŒ Failed to parse CrewAI result: {e}")
            import traceback
            logger.error(traceback.format_exc())
            parsed["executive_summary"] = raw_result[:1000] + "..."

        return parsed

    def _save_last_result(self, results: Dict[str, Any]) -> None:
        """ë§ˆì§€ë§‰ ë¶„ì„ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥ (raw crewai_resultë§Œ ì €ì¥)"""
        try:
            cache_dir = "/app/results"
            os.makedirs(cache_dir, exist_ok=True)
            cache_file = os.path.join(cache_dir, "last_analysis_result.json")

            # Raw crewai_resultì™€ ë©”íƒ€ë°ì´í„°ë§Œ ì €ì¥ (íŒŒì‹± ê²°ê³¼ëŠ” ì €ì¥í•˜ì§€ ì•ŠìŒ)
            cache_data = {
                "crewai_result": results.get("crewai_result", ""),
                "workflow_metadata": results.get("workflow_metadata", {}),
                "performance_metrics": results.get("performance_metrics", {}),
                "github_repo_url": results.get("github_repo_url", ""),
                "success": results.get("success", False)
            }

            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, indent=2, default=str, ensure_ascii=False)

            logger.info(f"ğŸ’¾ Raw analysis result cached to {cache_file}")
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to cache result: {e}")

    def _load_last_result(self) -> Optional[Dict[str, Any]]:
        """ìºì‹œëœ raw resultë¥¼ ë¡œë“œí•˜ê³  ë¬´ì¡°ê±´ ìƒˆë¡œ íŒŒì‹±"""
        try:
            cache_file = "/app/results/last_analysis_result.json"
            if os.path.exists(cache_file):
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cached_data = json.load(f)

                crewai_result = cached_data.get("crewai_result", "")

                if not crewai_result or len(crewai_result) < 100:
                    logger.warning("âš ï¸ Cached result is empty or too short")
                    return None

                logger.info("ğŸ“‚ Loaded cached raw result, parsing now...")

                # ë¬´ì¡°ê±´ ìƒˆë¡œ íŒŒì‹± (ìµœì‹  íŒŒì‹± ë¡œì§ ì ìš©)
                parsed_result = self._parse_crewai_result(crewai_result)

                # ì™„ì „í•œ ê²°ê³¼ êµ¬ì¡° ìƒì„±
                result = {
                    "workflow_metadata": cached_data.get("workflow_metadata", {}),
                    "security_analysis": parsed_result.get("security_analysis", {}),
                    "remediation_plan": parsed_result.get("remediation_plan", {}),
                    "final_report": parsed_result.get("final_report", {}),
                    "executive_summary": parsed_result.get("executive_summary", crewai_result[:500] + "..."),
                    "crewai_result": crewai_result,
                    "success": cached_data.get("success", False),
                    "performance_metrics": cached_data.get("performance_metrics", {}),
                    "github_repo_url": cached_data.get("github_repo_url", "")
                }

                vuln_count = result['security_analysis']['analysis_summary']['total_vulnerabilities']
                logger.info(f"   âœ… Parsing complete: {vuln_count} vulnerabilities found")

                return result
            else:
                logger.info("â„¹ï¸ No cached result found")
                return None
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to load cached result: {e}")
            return None

    def _create_error_result(self, error_message: str, project_path: str, *args) -> Dict[str, Any]:
        """ì—ëŸ¬ ê²°ê³¼ ìƒì„±"""
        return {
            "error": error_message,
            "project_path": project_path,
            "timestamp": datetime.now().isoformat(),
            "success": False
        }

    def get_workflow_summary(self) -> str:
        """ì›Œí¬í”Œë¡œìš° ìš”ì•½ ìƒì„±"""

        if not self.workflow_results:
            return "No workflow results available."

        performance = self.performance_metrics
        metadata = self.workflow_results.get("workflow_metadata", {})

        return f"""
ğŸ¯ CrewAI Workflow Summary:
  - Total Duration: {performance.get('total_duration', 0):.1f} seconds
  - Orchestration Mode: {metadata.get('orchestration_mode', 'crewai')}
  - Agents Used: {', '.join(performance.get('agents_used', []))}
  - Success: {'âœ…' if self.workflow_results.get('success') else 'âŒ'}

âœ… Ready for review!
"""

    def save_results(self, output_file: str = None) -> str:
        """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""

        if not self.workflow_results:
            return "No results to save."

        output_file = output_file or f"security_analysis_{int(time.time())}.json"

        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(self.workflow_results, f, indent=2, ensure_ascii=False, default=str)

            return f"Results saved to {output_file}"

        except Exception as e:
            return f"Failed to save results: {str(e)}"